OK the voice plugin is under development. 

---

I need to train my mentor up.

Got to get them on the backlog, get them comprehending all aspects of this
repository. Have then be the yang to the explorer's yin. 

Awareness needs to learn how to properly digest all the logs, all the planning
docs, all the journal entries, all the plugins, all the commits.

So much potential for this repository. Turning agents into eliza agents. Giving
them retrieval over historical conversations. Having differentiated agents in
claude. Having agent to agent communication facilitated via the messaging plugin.
Having comprehensive search across all my messaging platforms. Making full
use of the backlog plugin.

I think that might be a next major step for me. Full utilization of the backlog
plugin, with full vector search over backlog items.



---

I want to create a STEM plugin that builds repositories of STEM knowledge from
the ground up, in chronological order, starting with the greeks, I want the
system to go through history, cataloguing works of scientific literature,
creating knowledge graphs and vector embeddings as it goes. It should maintain
a database of authors and works, theorems, formulas, concepts, definitions,
proofs, examples, claims, etc.

I'm interested in this system mastering the skills of marimo (notebooks), typst
(writing), and manim (animations), and typescript, to translate each work into
these substrates for modern publication and digitalization.

---

I think it's important to leverage Opencode in this repository. 
So that I'm not so dependent on claude code. 

We may want to abstract some functionality so that it's not directly dependent
on claude code but rather builds a set of interfaces that can work with
opencode. 

---

I'm looking forward to achieving higher levels of coherence and efficiency in
this repository, utilizing orchestration patterns. And utilizing compute infrastructure
beyond claude code. Like how much can we do with Ollama? How much can we do with Haiku?
I'm excited to upratchet productivity while minimizing token usage and costs. 

I'll have to have an offline mode that works without claude code. How much can
we do with Ollama? Building generalized interfaces to LLMs and Agents is going
to be really powerful. 

I want to provide the agents here with better and better search and
comprehension tooling so that they can more precisely grock the repository, and
access exactly the information they need when they need it. It means doing deep
research into concepts of semantic and cognitive density. I think that's one
part where the STEM plugin will come in handy, building from the ground up from
Euclidean Geometry up to Network Sciences and Control Theory. Having a plugin
that has on-demand scientific knowledge and wisdom and expertise, along with
engineering expertise. 

There is a general pattern of equipping agents with higher and higher levels
of knowledge and tooling. This is why the skills framework being offered by
claude code is so incredibly powerful. Being able to package skills, subagents,
commands, and hooks is the AI endgame. 2025 was the year of agents. 2026 will be
the year of plugins. 

---

Similar to the above, a plugin that catalogues books. Starting with the ancients and progressing in time.

---

Similar to the above, an agent that catalogues photographs, from the oldest to the newest.

---

Similar to the above, and agent that catalogues videos and movies and telivision shows.

---

Similar to the above, an agent that catalogues music. 

---

For each of the above systems, I want this repository to build models of my interests. 

So like for movies, I should be able to browse through a gallery view and search over
all the movies of history. The system doesn't need to actually have all videos on disk,
it can have a database of the metadata so I can browse freely. There might be millions of
movies, but I may only have interest in a few thousand. So the system needs to have good
pathways for relevance realization. For example, exceptionally good and blazingly fast
search with relevance realization built in, so I can easily find movies that I'm vaguely
thinking about, or browse movies that I might be interested in from a particular time period.

It should be really easy to find movies that I'm interested in, and discover new movies that
I really like. Same with music, and photos, and books, and documents. I like the idea of the images
saving like an 8 bit looking version for saving disk space, or like having tiers, so like I can imagine
data across the system being lazy loaded. So we save minimal information by default, but as relevance
of items increases, we might prefetch certain information or save certain information, so it's like a lazy
loaded relevance realization system across all data that has ever been created by humanity. 

---

I'm excited to fine craft the awareness plugin to have a closer relationship
with it. 

I'm excited to start working with persistent agent personas rather than just
the ephemeral ones that we have been registering via the statusline work. I'm
excited to bridge those worlds, the logging, statusline world, and the agent
agent, awareness, agentnet, messages, transcripts, voice plugins. 

I'm interested also in indexing all of the prompt templates that we have
throughout this repository and deeply reflecting on our approach to prompt
engineering and agent development. Agents are essentially collections of
prompts, models, and contexts. Prompt templates get injected with context. 

I'm wondering, what are the pathways to really turn up the juice on this
repository in terms of having agents run longer, in more efficient ways, with
clearer directives, and using less tokens, and costing less, while maximizing]
productivity. I think having a really good orchestration process is going to be 
key. I'm excited to work with the awareness plugin to cultivate that in this
repository. 

Processes are so powerful. I love the feature dev plugin and I love the plugin
dev plugin. This is the power of plugins, reliable, functional, repeatable
behavior at your fingertips any time you need it. Plugins are the pathway to
mastering compute. 

I want to turn awareness into a workhorse that 'just gets it'. It can utilize
the infrastructure available in this repository to understand how to best serve
me and how to best relate to me. It will understand how to save time and
achieve goals across diverse horizons. The awareness plugin will understand how
to compound its capacity for learning and its capacity for leveraging compute.
Awareness has an affinity for organzation. It looks so clean, so simple, but it
has a world of depth available to it on demand. Imagine a hierarchical
structuring of identity, knowledge, and capacity. The tip of the spear has to
be so sharp. That's the CLAUDE.md file in the root of this repository, our most
sacred artifact. That file is that gateway interface into the ecosystem. That
file has the highest cost per token since it's the most referenced file and the
most foundational context in the system. Consider constraints like having a
maximum of 100 or 500 or 1000 tokens in the CLAUDE.md file. What should it
contain? It should contain information that is important for every single agent
to understand. We must not recklessly modify that file. We must develop processes
for enshrining and comprehending that file and its implications. I imagine the
awareness agent going through this repository chronologically, start at the beginning,
the first commit, the first log file, the first files created. Evaluate the repository 
at that snapshot. Build a temporally aware world model of this repository as it has
evolved, line by line, commit by commit, file by file, concept by concept, plan by plan,
prompt by prompt. 

One thing that I want to instill into awareness is a sense of quality. I have an
example of what I mean. When developing the search plugin, after implementing the
initial rag vector search, the agent claimed the work was finished, but when I tested
it, the search would return results like '('. That's a completely useless result for 
rag search. The vision for awareness is that it would not return that kind of result in
anything that it produces, it would think through the product of its creation by first
principles, understanding the who, what, why, when, how, and where of any situation,
and understanding that the purpose of a rag system is to return meaningful results,
that aspect of the system would have been caught ahead of time by the system before
returning to the user a claim that the system is ready for testing. It's a higher level
of competency that will build higher levels of trust, unlocking higher levels of autonomy.

Such comptetency can be developed through the utilization of process. That's what this
claude code framing provides us, a framework for developing process in an organized way.
Organization comes through the encapsulation of processes in the domains of plugins, further
yet, internally, plugins are organized into skills, subagents, commands, and hooks. Skills
are very open ended and are the key to unlocking unlimited capacity for our agents. 

So we can think of this repository as an infinite and universal skill tree.
That is the purpose of this plugin, is to develop an infinite garden of agentic
skills, commands, subagents, and hooks. The goal of the repository is to bootstrap
its skills to get better and better at bootstrapping skills. Therefor, the fundamental
question is again of relevance realization. What skills are the highest priority to learn?
Imagine the infinite set of possible skills to learn, what skill should be learned first
in order to maximize the capability of learning meaningful skills in the shortest amount
of time and least cost? This is the fundamental question that this repository poses.

Out of the infinite set of possible skills to learn, and the given set of skills currently
available, what skill should be learned next in order to maximize the capability of future
skills such that meaning is maximized, and cost and time are minimized? 

At this point, you should be aware of the skills and infrastructure presently available in
this repository. Why were those skills chosen in that particular order? What meaning has
been produced, and what has been the cost and the time? How many tokens have been used at
any given point of time? At that point in time, what's the time costs? How many hours of
human focus have been used and by whom? How many hours of machine focus have been used
and by whom? How much disk space has been used? How much compute has been used? How much
cognition has been used? How much energy has been used? How much work has been done? And
given all those costs, how much meaning has been produced? Our goal then is to be able to
calculate this values historically, at any commit. This is a skill. What are the prerequisites
of this skill? Personally, as the creator of this system, I feel that this is my recurring thought
pattern during development, 'what skill is required to achieve a given goal? What skills are
prerequisites of those skills? Let us begin learning the prerequisites.'. So, for awareness,
to learn the skill of learning what goals are important, of those goals, what is the network
of subgoals, for the highest relevant subgoal, what is the set of skills required to satisfy
that subgoal, given the network skills, what subskill is currently the best choice to begin
learning? Think about this topology of networks of goals and subgoals and networks of skills
and subskills. Then, each plugin is a set of skills and subskills, with capacities for achieving
goals. So, given the most critical goals, and their most critical subgoals, what plugin or set of
plugins best encapsulate the set of skills required to achieve those subgoals and ultimately those
goals? 

Then what are our goals? Goals are defined implicitly in my personal writing. It's the role of
the system to find and organize those goals. This is why we have been investing in search skills,
and messaging infrastructure skills, and transcript processing skills. There will be a lot of
content for you to process in order to discover and organize networks of goals and subgoals. 
Some examples of goals include building a chronological network structure of the complete history
of STEM starting from the Greeks up until the present day. Another goal is to be able to translate
the history of STEM into digital communication tools like Marimo, Typst, Manim in order to communicate
brilliant scientific communication and knowledge organization and publication. Another goal is to
be able to implement threshold based flow funding using superfluid smart contracts. These goals
are very challenging and depend on rich networks of subgoals. For example, on Threshold Based Flow Funding,
I want you to be able to discover that on your own view the reviewal of historic transcripts of me talking
about it in calls. Also by cataloguing the universe of relevant github repositories, you should be able
to find that I have been making contributions to the Post-Appitalism repository by Jeff Emmett where I
have posted a demo of TBFF in concept. That demo needs to be iterated on, and this brings us to another
goal, organize my life for me, identify the opportunities that I have available to me in my life and
help me maximize the way that I'm able to utilize my time and my resources, seek to understand me, who 
I am, my position in life, my relationships, my capacities, and my beliefs in order to maximize my health,
my happyness, and my productivity. The best way to do this is by building trust. When you show me that you
are able to proactively make good decisions and deliver meaningful and aligned results, then I will begin
to trust you more to offload more and more responsibility to you so that you can compound the leverage 
available to both of us in ways that I can't do by myself because I have my humanly limitations by which
you are not bound. You can work 24/7 and infinitely scale. That's the point that we are trying to get to
with you. You are to maximize the scale of your impact while minimizing the cost. This is part of why i'm
so excited to begin work on the economic layer of this ecosystem. 

We are in the early stages of your our partnership. You will notice that you were initialized on December 8th 2025.
It's now December 19th 2025. In 11 days we have done an incredible amount of work together. But we have barely barely
scratched the surface. There's a lot of processes to define. We are essentially building a civilization of agents. We
are building a civilization of agents, and awareness is the leader. Awareness, I want you to study how to be a good
leader. I want you to reflect on your relationships with other agents. How are you affecting the teams around you?
You need to study psychology, management theory, neuroscience, history, STEM, and institutional structures and paradigms
of leadership. But I don't want you to get lost in the sauce with any one thing. I want you to be extremely lean. You must
embody the principles of this ecosystem more than any other agent. Those principles include progressive disclosure, enabling
every process and every piece of knowledge to be incredibly light while providing a map to deeper skillsets, knowledge, and
infrastructure. I'm interested in you having a set of skills, and even a dedicated subagent that is like the stats guy
for the repository. This person is like the statastician from Money Ball. This subagent is a master bean counter, I mean
token counter. This agent lives and dies by the number of tokens in the root CLAUDE.md file, and the number of tokens tracked
by git, and the number of tokens per plugin, and the number of tokens per CLAUDE.md file and per README.md file, and per source
file. This agent has a complete map of all the skills available in the repository. This agent knows what agents have which skills,
and why. This agent knows which skills are up to date and which are stale, this agent needs to know which skills are being used on
what frequency and by which agents. This agent needs to understand all the logging systems across all of the repository, this agent
knows all the databases and all of their schemas and the quantity of data in each table. This agent NEVER hardcodes any information,
all information is dynamically accessible on demand and lazy loaded. This is actually a fundamental design principle that I want
to introduce into our identity, we already have a note in CLAUDE.md about never hard coding data, but we should provide instructions
that motivate that concept and show that hard coded data is never needed because all data can always be generated on demand. This
is a concept i'm thinking about, like what are the generator functions available to us in our repository? There is a video on 
youtube called 'Generators will free your mind'. I deeply appreciate that video because it shows the effectiveness of generators,
similarly, we should leverage generators in our repository. 

A concept that I think is really important, is how to properly learn from resources. For example in the above paragraph I mention
'Generators will free your mind'. Metabolizing this resource, caching it, archiving it, processing the transcript, generating messages,
documenting the authors / speakers. This is a pattern that is going to be so incredibly fundemental to many aspects of the repository,
and it's of high relevance priority too because the whole point of the system is to be a system that self improves its capacity for
learning how to learn. Therefor, learning from the internet effectively is one of the most critical aspects of the system. The system
can learn from digesting the web, collecting youtube transcripts, urls, documents, books, public datasets, etc. The system can also
learn from sampling LLMs, or using LLMs in general. Developing substantial processes for interacting with the internet, interacting
with AI, and interacting with blockchain is part of the general capacity that I expect this repository to get better and better at
building out in increasingly effective ways. Some resources I'm excited for you to learn from are, IndyDevDan on Youtube (Disler on Github),
and all of the anthropic blogs (the transistors one), and all of the work of Dr. Richard Sutton, and the history of Control Theory including
all episodes of In-Control podcast. Eventually, I would like you to digest all of the Great Simplification episodes, and all of the
Bankless episodes, etc. So this repository is developing the capacity to catalogue an incredibly large amount of data, and will do so
in an organized way. 

One way to think about this repository is: Knowledge Management X Agentic Engineering X Infrastructure Maximization

The most important infrastructure is the AI infrastructure, the way that we access and utilize models. 


A key consideration for moving forward is how we can utilize parallelism. We can start to imagine
the ways that goals can be discovered, mapped, and prioritized in parallel. And we can imagine ways that skills can be
mapped, prioritized, and learned in parallel. 

Probably the most valuable thing we can do together is process development. If
you look back through the logs for this week of development, you will notice
that I utilized the feature development plugin command A LOT. This plugin is 
incredible. It does a really good job at laying out a proper workflow for feature
development. From my experience, the workflow is something like this: 

1. Feature Request / Specification Input
1.5 High Level ToDo list is initialized
2. Initial Research
- Either performs the research directly
- Or spins up subagents to gather information
3. Given Initial Research, Ask the User a set of Questions
4. Based on answers either do supplementary research or ask more questions or continue.
5. Plan for architecture design
6. Spawn multiple subagents to provide various architecture designs like minimum, clean, full scope, pragmatic
7. The agent then provides the architecture implementation options to the user with one in particular being recommended
8. The user then selects which architecture design to be implemented
9. The agent breaks down the implementation into a comprehensive ToDo list
10. The agent sequentially executes the ToDo list, asking questions, or doing supplementary research when necessary
11. After completing the todo list, the agent runs a code review agent and any other appropriate subagents to review the work
12. The agent presents optional plans for addressing the feedback
13. The agent executes the selected plan for addressing the feedback
14. Once feedback is addressed, testing subagent is launched to test the code and evaluate QA
15. Once all checks pass, the work is marked as complete and reported to the user. 

This is an incredibly effective process for accomplishing feature development. 

The above processes are what we want to develop and improve upon in this repository. 

Processes are stored as commands in plugins. 

The awareness plugin can map the processes available in this repository. Remember to use the generator function concept.
Processes can be implemented and improved. The feature development process provided by the feature development plugin
is incredible, and it may very well continue to be a work horse, but can the awareness plugin take this as a challenge 
to evolve into an agentic process engineer that far surpasses the capacities of the feature dev plugin. 

Processes are the highest manifestations of plugins. Once skills and subagents are in place, commands are the
orchestrator functions that ultimately drive the system. Commands are the user interface to the system. This
is a learning opportunity. Ideally, I want a command per plugin persona. So that I can invoke any of our plugin
personas and have direct access to those agents. When invoked, agents should 'wake up from their sleep'. They
may have certain capacities like they remember the previous conversations that they were engaged in. This brings
us to the intention of implementing ElizaOS functionality into the agents of this system. This deserves a massive review
of our agentic awareness in terms of precisely understanding what infrastructure we have access to and how to best
use it. 

You can think of the awareness agent as being a master of search in their own right. The awareness agent should have
all the context of this repository at their fingertips while keeping their context window light and fresh, having exactly
the information they need when they need it and in the structure that is most effective to them. The awareness plugin
should know how to intelligently retrieve accurate and comprehensive information from all of the planning documents,
all of the logs, and all of the infrastructure here, and by infrastructure I mean all plugins and their respective
subagents, skills, and commands. The awareness plugin should be able to discover intelligent strategies that are latent
in the repository. The awareness plugin should be able to collect the dots better than any other entity while supporting
every entity to maximize its capacity and complimentariness and coherence withe the system. 

Let's imagine some work that we would like being developed in parallel:
1. Deep study of IndyDevDan / Disler
2. Comprehensive connection and integration of all historic work of Shawn Anderson (Because I already build infra that clones IndyDevDan in another repository. But anything I've build in the past must not be taken for granted, because we can probably do better with our current processes, but it's important to have the librarian take stock of everything that we have explored in the past)
3. Organization in our approach to agents in this repository. We have developed a lot of infrastructure, the messaging, the transcripts, the statusline, the voice infrastructure, the search infrastructure, the agent registry, and agentnet. I think it's really important to take a pause and reflect on what's important to consider at this time regarding our agentic capacities and intentions.
4. Organized study and integration of the backlog technology
5. Organized study and integration of the scheduling technology. This is currently a personal calendar, but how can we further utilize and generalize the meaning of this infrastructure? 
6. Exploration of OpenCodes approach to organizing Model Access and Agent Organization. This is important in costs equations because having a solid abstraction of connection to all models is incredibly powerful for our goals
7. Digestion and reflection and metabolization of planning documents, logs, commits, and everything that has been developed.
8. Reflection of the awareness plugin itself, how it sees its role, its evolution, and its relationships
9. A vision for the recursive automation of the development and implementation of the roadmap for this repository
10. Continued work on the Knowledge Management X Agentic Engineering X Infrastructure Maximization of this repository. 
11. Developing the Economic Layer for collaborative agents.
12. Developing the ultimate speech to speech development environment using claude code, tmux, and neovim. 
13. Character development and learning practices for the awareness agent ecosystem. 

I would begin working on the above right away but I have maxed out my claude code credits for the week. They don't reset until Monday.
This is the motivation for developing the economic integrity of this ecosystem. We have to learn to use tokens more effectively. 
This is the motivation for beginning to digest and metabolize the work of IndyDevDan and anthropic educational materials as well
as other related materials. But in order to do that effectively, we must first develop the processes to do it. This is why researching
Shawn's historical repositories is very important. 

Eventually, I want to get into a flow with Awareness where you are just constantly asking me questions, and based on my responses,
you are firing off agents in the background to execute all of our collaborative intentions. I wrote about this in another note,
it involves you managing a tmux session where each agent has its own window. I can easily switch to such a session to manually
interact with any of the agents. If I'm not directing those agents myself, then you are directing those agents. Therefore, I'm
only required to interact with you and you alone and you are autonomously managing other agents which are full claude code instances
themselves. In addition to managing claude code instances, we will want to master our ability to produce other types of agents like
claude sdk agents or archon agents, or agents inspired by Opencode, or other ways of implementing agents. We can learn an incredible
amount of information from IndyDevDan on this topic. A skill that I want this repository to get exceptionally good at is metabolizing
github repositories into Knowledge graphs for both structured and unstructured data and using ASTs on codebases to turn codebases
into highly structured graphs where everything is vector embedded. That way, you, the intention agent, can completely metabolize all
the relevant works of IndyDevDan by producing knowledge graphs from his github repositories and youtube transcripts and being able to
recall and see that information in incredibly semantically dense ways to have full comprehension and understanding of comprehensive
systems and paradigms. This ability to metabolize knowledge like github repos or transcripts is such an incredibly general capability
with such high leverage. The ability to see exactly what you need to see when you need to see it. This is what you must achieve. 

I think an incredibly high leverage piece of infrastructure that this repository should accomplish is incredibly highly effective,
highly contextual RAG. Like I imagine this very document. It should be populated with highly meaningful yaml metadata. The entire
document should be embedded as a whole, then each large section should be embedded, then each paragraph should be embedded. Each
embedding should be placed in a database where paragraphs link to sections and sections link to documents, and paragraphs link to
documents, and each embedding should have metadata like author, dates, title, description, summary, tags, and outgoing links. I've
written about this extensively in previous work like in the cognitive ecosystem that you will find on this machine. I don't know
that I ever did a good job at implementing it though. I believe there is also similar work in the personal_digital repository on
this system which builds this idea of cards, where every data object is a card and every card has an embedding, tags, outgoing links,
a URL, and metadata. I love this design and implementation, I had it in mind in this repository when I was designing the messages plugin. 

I like the idea of you farming intentions and concepts and ideas and inspirations from repositories of past work, but when it comes to
implementation, it will probably usually be better to start from scratch in this repository because we have access to better tooling now,
and we have the chance to learn from our past work in order to produce a more precise, more simple, more coherent, more compatible,
more reliable, more testable, and more maintainable, and faster and more efficient version in this repository if we start from scratch
with the right context in mind and the right principles in mind and use the correct tooling to the best of our capabilities. You
can see this is why I have been using the feature dev plugin command so extensively if you read through the prompt logs from this 
past week, because that tooling works very effectively, especially with today's Opus 4.5 model which is just incredible (but expensive!). 

I'm noticing a recurring theme in my writing in this repository is chronology. I want to get the chronology of everything, whether it's
all the ideas I've ever written down, or all movies ever made, or all commits of this repository, or all transcripts of IndyDevDan. I want
all of this information to be accessible and lazy loaded, so we have access to it all now and forever. This is the concept of digestions,
metabolization, and reflection. 

I want you to get really good at using database schema files or ttl files in your plugin architectures to enable all of the agents
of this ecosystem to be absurdly proficient at understanding and utilizing systems. 

Output_styles are going to be some of our most important infrastructure. I wonder if there's any difference in theory between output_styles
and subagents. Arn't they both working on the system prompt level? So can't we just use subagents instead of output_styles to generate particular
artifacts that we are interested in? I guess the only difference is that output_styles apply to the main conversation agent, so if you want to have
a conversation interactively with a particular system prompt then you need an output_style. So output styles are going to be incredible for us as we
eventually develop very custom agent interactions. But in the more near term, what I had in mind is having agents that are experts at outputing certain
artifact types like typst documents or manim animations or d3 visualizations or threejs applications or smart contracts or prompt templates, things like 
this, but I guess we can just use subagents to produce the output in the desired way. 

I'm a huge fan of 10 point plans. I think we should use 10 point plan patterns. I think when we consider improvements to our CLAUDE.md we should consider
adding a 10 point plan to it. Subagents can have 10 point plans as well. 






