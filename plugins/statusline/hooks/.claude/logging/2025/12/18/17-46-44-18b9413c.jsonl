{"ts": "2025-12-18T17:46:44.678261", "type": "PreToolUse", "session_id": "18b9413c-d1b7-4db7-83e6-73cef8efe045", "agent_session": 0, "data": {"session_id": "18b9413c-d1b7-4db7-83e6-73cef8efe045", "transcript_path": "/home/ygg/.claude/projects/-home-ygg-Workspace-sandbox-marketplaces-claude/18b9413c-d1b7-4db7-83e6-73cef8efe045.jsonl", "cwd": "/home/ygg/Workspace/sandbox/marketplaces/claude/plugins/statusline/hooks", "permission_mode": "bypassPermissions", "hook_event_name": "PreToolUse", "tool_name": "Read", "tool_input": {"file_path": "/home/ygg/Workspace/sandbox/marketplaces/claude/plugins/transcripts/src/adapters/transcription/whisper.ts"}, "tool_use_id": "toolu_01NWQacePiR9nG78Ds87GX93"}}
{"ts": "2025-12-18T17:46:44.783415", "type": "PostToolUse", "session_id": "18b9413c-d1b7-4db7-83e6-73cef8efe045", "agent_session": 0, "data": {"session_id": "18b9413c-d1b7-4db7-83e6-73cef8efe045", "transcript_path": "/home/ygg/.claude/projects/-home-ygg-Workspace-sandbox-marketplaces-claude/18b9413c-d1b7-4db7-83e6-73cef8efe045.jsonl", "cwd": "/home/ygg/Workspace/sandbox/marketplaces/claude/plugins/statusline/hooks", "permission_mode": "bypassPermissions", "hook_event_name": "PostToolUse", "tool_name": "Read", "tool_input": {"file_path": "/home/ygg/Workspace/sandbox/marketplaces/claude/plugins/transcripts/src/adapters/transcription/whisper.ts"}, "tool_response": {"type": "text", "file": {"filePath": "/home/ygg/Workspace/sandbox/marketplaces/claude/plugins/transcripts/src/adapters/transcription/whisper.ts", "content": "/**\n * Whisper Transcription Adapter\n *\n * Implements TranscriptionPort for OpenAI Whisper (local and API).\n * Supports whisper.cpp for local inference and OpenAI API for cloud.\n */\n\nimport { spawn } from \"child_process\";\nimport { existsSync, statSync } from \"fs\";\nimport type {\n  TranscriptionPort,\n  TranscriptionCapabilities,\n  TranscriptionOptions,\n  TranscriptionResult,\n  TranscriptionProgressCallback,\n  StreamingEvent,\n} from \"../../ports/transcription.js\";\nimport type { AudioInput } from \"../../domain/values/media-source.js\";\nimport type { Utterance } from \"../../domain/entities/utterance.js\";\n\n/**\n * Whisper model sizes\n */\nexport type WhisperModel =\n  | \"tiny\"\n  | \"tiny.en\"\n  | \"base\"\n  | \"base.en\"\n  | \"small\"\n  | \"small.en\"\n  | \"medium\"\n  | \"medium.en\"\n  | \"large\"\n  | \"large-v2\"\n  | \"large-v3\";\n\n/**\n * Whisper backend mode\n */\nexport type WhisperMode = \"local\" | \"api\";\n\n/**\n * Whisper adapter configuration\n */\nexport interface WhisperConfig {\n  mode: WhisperMode;\n  model?: WhisperModel;\n\n  // Local mode settings\n  whisperCppPath?: string;      // Path to whisper.cpp binary\n  modelPath?: string;           // Path to model file\n\n  // API mode settings\n  apiKey?: string;              // OpenAI API key\n  apiBaseUrl?: string;          // Custom API base URL\n}\n\n/**\n * Default configuration\n */\nconst DEFAULT_CONFIG: WhisperConfig = {\n  mode: \"local\",\n  model: \"base\",\n  whisperCppPath: \"whisper\",    // Assumes in PATH\n};\n\n/**\n * Whisper.cpp JSON output segment\n */\ninterface WhisperSegment {\n  id: number;\n  seek: number;\n  start: number;\n  end: number;\n  text: string;\n  tokens: number[];\n  temperature: number;\n  avg_logprob: number;\n  compression_ratio: number;\n  no_speech_prob: number;\n}\n\n/**\n * Whisper.cpp JSON output\n */\ninterface WhisperOutput {\n  systeminfo: string;\n  model: {\n    type: string;\n    multilingual: boolean;\n    vocab: number;\n    audio: { ctx: number; state: number; head: number; layer: number };\n    text: { ctx: number; state: number; head: number; layer: number };\n    mels: number;\n    ftype: number;\n  };\n  params: {\n    model: string;\n    language: string;\n    translate: boolean;\n  };\n  result: {\n    language: string;\n  };\n  transcription: WhisperSegment[];\n}\n\n/**\n * Whisper Transcription Adapter\n */\nexport class WhisperAdapter implements TranscriptionPort {\n  private config: WhisperConfig;\n\n  constructor(config: Partial<WhisperConfig> = {}) {\n    this.config = { ...DEFAULT_CONFIG, ...config };\n  }\n\n  name(): string {\n    return `whisper-${this.config.mode}`;\n  }\n\n  capabilities(): TranscriptionCapabilities {\n    const isLocal = this.config.mode === \"local\";\n\n    return {\n      languages: [\n        \"en\", \"zh\", \"de\", \"es\", \"ru\", \"ko\", \"fr\", \"ja\", \"pt\", \"tr\",\n        \"pl\", \"ca\", \"nl\", \"ar\", \"sv\", \"it\", \"id\", \"hi\", \"fi\", \"vi\",\n        \"he\", \"uk\", \"el\", \"ms\", \"cs\", \"ro\", \"da\", \"hu\", \"ta\", \"no\",\n        \"th\", \"ur\", \"hr\", \"bg\", \"lt\", \"la\", \"mi\", \"ml\", \"cy\", \"sk\",\n        \"te\", \"fa\", \"lv\", \"bn\", \"sr\", \"az\", \"sl\", \"kn\", \"et\", \"mk\",\n        \"br\", \"eu\", \"is\", \"hy\", \"ne\", \"mn\", \"bs\", \"kk\", \"sq\", \"sw\",\n        \"gl\", \"mr\", \"pa\", \"si\", \"km\", \"sn\", \"yo\", \"so\", \"af\", \"oc\",\n        \"ka\", \"be\", \"tg\", \"sd\", \"gu\", \"am\", \"yi\", \"lo\", \"uz\", \"fo\",\n        \"ht\", \"ps\", \"tk\", \"nn\", \"mt\", \"sa\", \"lb\", \"my\", \"bo\", \"tl\",\n        \"mg\", \"as\", \"tt\", \"haw\", \"ln\", \"ha\", \"ba\", \"jw\", \"su\",\n      ],\n      auto_detect_language: true,\n      word_timestamps: true,\n      speaker_diarization: false,  // Whisper doesn't do diarization\n      punctuation: true,\n      profanity_filter: false,\n      supports_streaming: isLocal,  // Local can stream\n      supports_files: true,\n      supports_urls: !isLocal,      // API supports URLs\n      audio_formats: [\"wav\", \"mp3\", \"m4a\", \"flac\", \"ogg\", \"webm\"],\n      max_duration_ms: isLocal ? undefined : 25 * 60 * 1000,  // API has 25 min limit\n      models: [\n        \"tiny\", \"tiny.en\", \"base\", \"base.en\", \"small\", \"small.en\",\n        \"medium\", \"medium.en\", \"large\", \"large-v2\", \"large-v3\",\n      ],\n      default_model: \"base\",\n    };\n  }\n\n  async isAvailable(): Promise<boolean> {\n    if (this.config.mode === \"api\") {\n      return !!this.config.apiKey;\n    }\n\n    // Check if whisper.cpp is available\n    return new Promise((resolve) => {\n      const proc = spawn(this.config.whisperCppPath || \"whisper\", [\"--help\"]);\n      proc.on(\"error\", () => resolve(false));\n      proc.on(\"close\", (code) => resolve(code === 0));\n    });\n  }\n\n  async transcribe(\n    input: AudioInput,\n    options?: TranscriptionOptions,\n    onProgress?: TranscriptionProgressCallback\n  ): Promise<TranscriptionResult> {\n    if (this.config.mode === \"api\") {\n      return this.transcribeAPI(input, options);\n    }\n    return this.transcribeLocal(input, options, onProgress);\n  }\n\n  /**\n   * Local transcription using whisper.cpp\n   */\n  private async transcribeLocal(\n    input: AudioInput,\n    options?: TranscriptionOptions,\n    onProgress?: TranscriptionProgressCallback\n  ): Promise<TranscriptionResult> {\n    if (input.type !== \"file\") {\n      throw new Error(\"Local Whisper only supports file input\");\n    }\n\n    const startTime = Date.now();\n\n    // Build command arguments\n    const args: string[] = [\n      \"-f\", input.path,\n      \"-oj\",                      // JSON output\n      \"--print-progress\",\n    ];\n\n    if (options?.language) {\n      args.push(\"-l\", options.language);\n    }\n\n    if (options?.word_timestamps) {\n      args.push(\"--max-len\", \"0\");  // Word-level timestamps\n    }\n\n    if (options?.model) {\n      args.push(\"-m\", this.getModelPath(options.model));\n    } else if (this.config.modelPath) {\n      args.push(\"-m\", this.config.modelPath);\n    }\n\n    if (options?.beam_size) {\n      args.push(\"--beam-size\", String(options.beam_size));\n    }\n\n    if (options?.initial_prompt) {\n      args.push(\"--prompt\", options.initial_prompt);\n    }\n\n    // Run whisper\n    const result = await this.runWhisper(args, onProgress);\n\n    // Parse JSON output\n    const output: WhisperOutput = JSON.parse(result);\n\n    // Convert segments to utterances\n    const utterances: Utterance[] = output.transcription.map((seg, i) => ({\n      id: `ut_temp_${String(i).padStart(4, \"0\")}`,  // Temp ID, real one assigned by store\n      index: i,\n      speaker: {\n        id: \"spk_unknown\",\n        name: \"Speaker\",\n      },\n      text: seg.text.trim(),\n      start_ms: Math.round(seg.start * 1000),\n      end_ms: Math.round(seg.end * 1000),\n      duration_ms: Math.round((seg.end - seg.start) * 1000),\n      confidence: {\n        transcription: 1 - seg.no_speech_prob,\n      },\n      language: output.result.language,\n    }));\n\n    return {\n      utterances,\n      language: output.result.language,\n      language_confidence: undefined,  // Whisper doesn't provide this\n      duration_ms: utterances.length > 0\n        ? utterances[utterances.length - 1].end_ms\n        : 0,\n      processing_time_ms: Date.now() - startTime,\n      model: output.params.model || this.config.model || \"unknown\",\n    };\n  }\n\n  /**\n   * API transcription using OpenAI Whisper API\n   */\n  private async transcribeAPI(\n    input: AudioInput,\n    options?: TranscriptionOptions\n  ): Promise<TranscriptionResult> {\n    if (!this.config.apiKey) {\n      throw new Error(\"OpenAI API key required for API mode\");\n    }\n\n    const startTime = Date.now();\n\n    // Prepare form data\n    const formData = new FormData();\n\n    if (input.type === \"file\") {\n      const file = Bun.file(input.path);\n      formData.append(\"file\", file);\n    } else if (input.type === \"buffer\") {\n      const blob = new Blob([input.buffer], { type: `audio/${input.format}` });\n      formData.append(\"file\", blob, `audio.${input.format}`);\n    } else {\n      throw new Error(\"API mode requires file or buffer input\");\n    }\n\n    formData.append(\"model\", options?.model || \"whisper-1\");\n    formData.append(\"response_format\", \"verbose_json\");\n\n    if (options?.language) {\n      formData.append(\"language\", options.language);\n    }\n\n    if (options?.initial_prompt) {\n      formData.append(\"prompt\", options.initial_prompt);\n    }\n\n    if (options?.temperature !== undefined) {\n      formData.append(\"temperature\", String(options.temperature));\n    }\n\n    // Call API\n    const baseUrl = this.config.apiBaseUrl || \"https://api.openai.com/v1\";\n    const response = await fetch(`${baseUrl}/audio/transcriptions`, {\n      method: \"POST\",\n      headers: {\n        \"Authorization\": `Bearer ${this.config.apiKey}`,\n      },\n      body: formData,\n    });\n\n    if (!response.ok) {\n      const error = await response.text();\n      throw new Error(`Whisper API error: ${error}`);\n    }\n\n    const data = await response.json() as {\n      text: string;\n      language: string;\n      duration: number;\n      segments: Array<{\n        id: number;\n        start: number;\n        end: number;\n        text: string;\n        avg_logprob: number;\n        no_speech_prob: number;\n      }>;\n    };\n\n    // Convert to utterances\n    const utterances: Utterance[] = data.segments.map((seg, i) => ({\n      id: `ut_temp_${String(i).padStart(4, \"0\")}`,\n      index: i,\n      speaker: {\n        id: \"spk_unknown\",\n        name: \"Speaker\",\n      },\n      text: seg.text.trim(),\n      start_ms: Math.round(seg.start * 1000),\n      end_ms: Math.round(seg.end * 1000),\n      duration_ms: Math.round((seg.end - seg.start) * 1000),\n      confidence: {\n        transcription: 1 - seg.no_speech_prob,\n      },\n      language: data.language,\n    }));\n\n    return {\n      utterances,\n      language: data.language,\n      duration_ms: Math.round(data.duration * 1000),\n      processing_time_ms: Date.now() - startTime,\n      model: options?.model || \"whisper-1\",\n    };\n  }\n\n  /**\n   * Run whisper.cpp and capture output\n   */\n  private runWhisper(\n    args: string[],\n    onProgress?: TranscriptionProgressCallback\n  ): Promise<string> {\n    return new Promise((resolve, reject) => {\n      const proc = spawn(this.config.whisperCppPath || \"whisper\", args);\n\n      let stdout = \"\";\n      let stderr = \"\";\n\n      proc.stdout.on(\"data\", (data) => {\n        stdout += data.toString();\n      });\n\n      proc.stderr.on(\"data\", (data) => {\n        const text = data.toString();\n        stderr += text;\n\n        // Parse progress from stderr\n        if (onProgress) {\n          const match = text.match(/progress\\s*=\\s*(\\d+)%/);\n          if (match) {\n            onProgress({\n              percent: parseInt(match[1], 10),\n              current_time_ms: 0,\n              total_time_ms: 0,\n              utterances_processed: 0,\n            });\n          }\n        }\n      });\n\n      proc.on(\"error\", (error) => {\n        reject(new Error(`Failed to run whisper: ${error.message}`));\n      });\n\n      proc.on(\"close\", (code) => {\n        if (code === 0) {\n          resolve(stdout);\n        } else {\n          reject(new Error(`Whisper exited with code ${code}: ${stderr}`));\n        }\n      });\n    });\n  }\n\n  /**\n   * Get model file path\n   */\n  private getModelPath(model: string): string {\n    if (this.config.modelPath) {\n      return this.config.modelPath;\n    }\n    // Default whisper.cpp model location\n    return `ggml-${model}.bin`;\n  }\n\n  /**\n   * Streaming transcription (local only)\n   */\n  async *transcribeStream(\n    input: AudioInput,\n    options?: TranscriptionOptions\n  ): AsyncGenerator<StreamingEvent> {\n    if (this.config.mode === \"api\") {\n      throw new Error(\"Streaming not supported in API mode\");\n    }\n\n    // For now, fall back to batch and emit as single final\n    yield { type: \"started\", session_id: `whisper_${Date.now()}` };\n\n    try {\n      const result = await this.transcribeLocal(input, options);\n\n      for (const utterance of result.utterances) {\n        yield { type: \"final\", utterance };\n      }\n\n      yield { type: \"completed\", result };\n    } catch (error) {\n      yield { type: \"error\", error: error as Error };\n    }\n  }\n}\n\n/**\n * Create a Whisper adapter\n */\nexport function createWhisperAdapter(config?: Partial<WhisperConfig>): WhisperAdapter {\n  return new WhisperAdapter(config);\n}\n", "numLines": 450, "startLine": 1, "totalLines": 450}}, "tool_use_id": "toolu_01NWQacePiR9nG78Ds87GX93"}}
{"ts": "2025-12-18T17:46:50.273839", "type": "PreToolUse", "session_id": "18b9413c-d1b7-4db7-83e6-73cef8efe045", "agent_session": 0, "data": {"session_id": "18b9413c-d1b7-4db7-83e6-73cef8efe045", "transcript_path": "/home/ygg/.claude/projects/-home-ygg-Workspace-sandbox-marketplaces-claude/18b9413c-d1b7-4db7-83e6-73cef8efe045.jsonl", "cwd": "/home/ygg/Workspace/sandbox/marketplaces/claude/plugins/statusline/hooks", "permission_mode": "bypassPermissions", "hook_event_name": "PreToolUse", "tool_name": "Edit", "tool_input": {"file_path": "/home/ygg/Workspace/sandbox/marketplaces/claude/plugins/transcripts/src/adapters/transcription/whisper.ts", "old_string": "import { spawn } from \"child_process\";\nimport { existsSync, statSync } from \"fs\";", "new_string": "import { spawn } from \"child_process\";"}, "tool_use_id": "toolu_014xK1tnhjWvzaUKYgeH1jaX"}}
{"ts": "2025-12-18T17:46:50.386763", "type": "PostToolUse", "session_id": "18b9413c-d1b7-4db7-83e6-73cef8efe045", "agent_session": 0, "data": {"session_id": "18b9413c-d1b7-4db7-83e6-73cef8efe045", "transcript_path": "/home/ygg/.claude/projects/-home-ygg-Workspace-sandbox-marketplaces-claude/18b9413c-d1b7-4db7-83e6-73cef8efe045.jsonl", "cwd": "/home/ygg/Workspace/sandbox/marketplaces/claude/plugins/statusline/hooks", "permission_mode": "bypassPermissions", "hook_event_name": "PostToolUse", "tool_name": "Edit", "tool_input": {"file_path": "/home/ygg/Workspace/sandbox/marketplaces/claude/plugins/transcripts/src/adapters/transcription/whisper.ts", "old_string": "import { spawn } from \"child_process\";\nimport { existsSync, statSync } from \"fs\";", "new_string": "import { spawn } from \"child_process\";"}, "tool_response": {"filePath": "/home/ygg/Workspace/sandbox/marketplaces/claude/plugins/transcripts/src/adapters/transcription/whisper.ts", "oldString": "import { spawn } from \"child_process\";\nimport { existsSync, statSync } from \"fs\";", "newString": "import { spawn } from \"child_process\";", "originalFile": "/**\n * Whisper Transcription Adapter\n *\n * Implements TranscriptionPort for OpenAI Whisper (local and API).\n * Supports whisper.cpp for local inference and OpenAI API for cloud.\n */\n\nimport { spawn } from \"child_process\";\nimport { existsSync, statSync } from \"fs\";\nimport type {\n  TranscriptionPort,\n  TranscriptionCapabilities,\n  TranscriptionOptions,\n  TranscriptionResult,\n  TranscriptionProgressCallback,\n  StreamingEvent,\n} from \"../../ports/transcription.js\";\nimport type { AudioInput } from \"../../domain/values/media-source.js\";\nimport type { Utterance } from \"../../domain/entities/utterance.js\";\n\n/**\n * Whisper model sizes\n */\nexport type WhisperModel =\n  | \"tiny\"\n  | \"tiny.en\"\n  | \"base\"\n  | \"base.en\"\n  | \"small\"\n  | \"small.en\"\n  | \"medium\"\n  | \"medium.en\"\n  | \"large\"\n  | \"large-v2\"\n  | \"large-v3\";\n\n/**\n * Whisper backend mode\n */\nexport type WhisperMode = \"local\" | \"api\";\n\n/**\n * Whisper adapter configuration\n */\nexport interface WhisperConfig {\n  mode: WhisperMode;\n  model?: WhisperModel;\n\n  // Local mode settings\n  whisperCppPath?: string;      // Path to whisper.cpp binary\n  modelPath?: string;           // Path to model file\n\n  // API mode settings\n  apiKey?: string;              // OpenAI API key\n  apiBaseUrl?: string;          // Custom API base URL\n}\n\n/**\n * Default configuration\n */\nconst DEFAULT_CONFIG: WhisperConfig = {\n  mode: \"local\",\n  model: \"base\",\n  whisperCppPath: \"whisper\",    // Assumes in PATH\n};\n\n/**\n * Whisper.cpp JSON output segment\n */\ninterface WhisperSegment {\n  id: number;\n  seek: number;\n  start: number;\n  end: number;\n  text: string;\n  tokens: number[];\n  temperature: number;\n  avg_logprob: number;\n  compression_ratio: number;\n  no_speech_prob: number;\n}\n\n/**\n * Whisper.cpp JSON output\n */\ninterface WhisperOutput {\n  systeminfo: string;\n  model: {\n    type: string;\n    multilingual: boolean;\n    vocab: number;\n    audio: { ctx: number; state: number; head: number; layer: number };\n    text: { ctx: number; state: number; head: number; layer: number };\n    mels: number;\n    ftype: number;\n  };\n  params: {\n    model: string;\n    language: string;\n    translate: boolean;\n  };\n  result: {\n    language: string;\n  };\n  transcription: WhisperSegment[];\n}\n\n/**\n * Whisper Transcription Adapter\n */\nexport class WhisperAdapter implements TranscriptionPort {\n  private config: WhisperConfig;\n\n  constructor(config: Partial<WhisperConfig> = {}) {\n    this.config = { ...DEFAULT_CONFIG, ...config };\n  }\n\n  name(): string {\n    return `whisper-${this.config.mode}`;\n  }\n\n  capabilities(): TranscriptionCapabilities {\n    const isLocal = this.config.mode === \"local\";\n\n    return {\n      languages: [\n        \"en\", \"zh\", \"de\", \"es\", \"ru\", \"ko\", \"fr\", \"ja\", \"pt\", \"tr\",\n        \"pl\", \"ca\", \"nl\", \"ar\", \"sv\", \"it\", \"id\", \"hi\", \"fi\", \"vi\",\n        \"he\", \"uk\", \"el\", \"ms\", \"cs\", \"ro\", \"da\", \"hu\", \"ta\", \"no\",\n        \"th\", \"ur\", \"hr\", \"bg\", \"lt\", \"la\", \"mi\", \"ml\", \"cy\", \"sk\",\n        \"te\", \"fa\", \"lv\", \"bn\", \"sr\", \"az\", \"sl\", \"kn\", \"et\", \"mk\",\n        \"br\", \"eu\", \"is\", \"hy\", \"ne\", \"mn\", \"bs\", \"kk\", \"sq\", \"sw\",\n        \"gl\", \"mr\", \"pa\", \"si\", \"km\", \"sn\", \"yo\", \"so\", \"af\", \"oc\",\n        \"ka\", \"be\", \"tg\", \"sd\", \"gu\", \"am\", \"yi\", \"lo\", \"uz\", \"fo\",\n        \"ht\", \"ps\", \"tk\", \"nn\", \"mt\", \"sa\", \"lb\", \"my\", \"bo\", \"tl\",\n        \"mg\", \"as\", \"tt\", \"haw\", \"ln\", \"ha\", \"ba\", \"jw\", \"su\",\n      ],\n      auto_detect_language: true,\n      word_timestamps: true,\n      speaker_diarization: false,  // Whisper doesn't do diarization\n      punctuation: true,\n      profanity_filter: false,\n      supports_streaming: isLocal,  // Local can stream\n      supports_files: true,\n      supports_urls: !isLocal,      // API supports URLs\n      audio_formats: [\"wav\", \"mp3\", \"m4a\", \"flac\", \"ogg\", \"webm\"],\n      max_duration_ms: isLocal ? undefined : 25 * 60 * 1000,  // API has 25 min limit\n      models: [\n        \"tiny\", \"tiny.en\", \"base\", \"base.en\", \"small\", \"small.en\",\n        \"medium\", \"medium.en\", \"large\", \"large-v2\", \"large-v3\",\n      ],\n      default_model: \"base\",\n    };\n  }\n\n  async isAvailable(): Promise<boolean> {\n    if (this.config.mode === \"api\") {\n      return !!this.config.apiKey;\n    }\n\n    // Check if whisper.cpp is available\n    return new Promise((resolve) => {\n      const proc = spawn(this.config.whisperCppPath || \"whisper\", [\"--help\"]);\n      proc.on(\"error\", () => resolve(false));\n      proc.on(\"close\", (code) => resolve(code === 0));\n    });\n  }\n\n  async transcribe(\n    input: AudioInput,\n    options?: TranscriptionOptions,\n    onProgress?: TranscriptionProgressCallback\n  ): Promise<TranscriptionResult> {\n    if (this.config.mode === \"api\") {\n      return this.transcribeAPI(input, options);\n    }\n    return this.transcribeLocal(input, options, onProgress);\n  }\n\n  /**\n   * Local transcription using whisper.cpp\n   */\n  private async transcribeLocal(\n    input: AudioInput,\n    options?: TranscriptionOptions,\n    onProgress?: TranscriptionProgressCallback\n  ): Promise<TranscriptionResult> {\n    if (input.type !== \"file\") {\n      throw new Error(\"Local Whisper only supports file input\");\n    }\n\n    const startTime = Date.now();\n\n    // Build command arguments\n    const args: string[] = [\n      \"-f\", input.path,\n      \"-oj\",                      // JSON output\n      \"--print-progress\",\n    ];\n\n    if (options?.language) {\n      args.push(\"-l\", options.language);\n    }\n\n    if (options?.word_timestamps) {\n      args.push(\"--max-len\", \"0\");  // Word-level timestamps\n    }\n\n    if (options?.model) {\n      args.push(\"-m\", this.getModelPath(options.model));\n    } else if (this.config.modelPath) {\n      args.push(\"-m\", this.config.modelPath);\n    }\n\n    if (options?.beam_size) {\n      args.push(\"--beam-size\", String(options.beam_size));\n    }\n\n    if (options?.initial_prompt) {\n      args.push(\"--prompt\", options.initial_prompt);\n    }\n\n    // Run whisper\n    const result = await this.runWhisper(args, onProgress);\n\n    // Parse JSON output\n    const output: WhisperOutput = JSON.parse(result);\n\n    // Convert segments to utterances\n    const utterances: Utterance[] = output.transcription.map((seg, i) => ({\n      id: `ut_temp_${String(i).padStart(4, \"0\")}`,  // Temp ID, real one assigned by store\n      index: i,\n      speaker: {\n        id: \"spk_unknown\",\n        name: \"Speaker\",\n      },\n      text: seg.text.trim(),\n      start_ms: Math.round(seg.start * 1000),\n      end_ms: Math.round(seg.end * 1000),\n      duration_ms: Math.round((seg.end - seg.start) * 1000),\n      confidence: {\n        transcription: 1 - seg.no_speech_prob,\n      },\n      language: output.result.language,\n    }));\n\n    return {\n      utterances,\n      language: output.result.language,\n      language_confidence: undefined,  // Whisper doesn't provide this\n      duration_ms: utterances.length > 0\n        ? utterances[utterances.length - 1].end_ms\n        : 0,\n      processing_time_ms: Date.now() - startTime,\n      model: output.params.model || this.config.model || \"unknown\",\n    };\n  }\n\n  /**\n   * API transcription using OpenAI Whisper API\n   */\n  private async transcribeAPI(\n    input: AudioInput,\n    options?: TranscriptionOptions\n  ): Promise<TranscriptionResult> {\n    if (!this.config.apiKey) {\n      throw new Error(\"OpenAI API key required for API mode\");\n    }\n\n    const startTime = Date.now();\n\n    // Prepare form data\n    const formData = new FormData();\n\n    if (input.type === \"file\") {\n      const file = Bun.file(input.path);\n      formData.append(\"file\", file);\n    } else if (input.type === \"buffer\") {\n      const blob = new Blob([input.buffer], { type: `audio/${input.format}` });\n      formData.append(\"file\", blob, `audio.${input.format}`);\n    } else {\n      throw new Error(\"API mode requires file or buffer input\");\n    }\n\n    formData.append(\"model\", options?.model || \"whisper-1\");\n    formData.append(\"response_format\", \"verbose_json\");\n\n    if (options?.language) {\n      formData.append(\"language\", options.language);\n    }\n\n    if (options?.initial_prompt) {\n      formData.append(\"prompt\", options.initial_prompt);\n    }\n\n    if (options?.temperature !== undefined) {\n      formData.append(\"temperature\", String(options.temperature));\n    }\n\n    // Call API\n    const baseUrl = this.config.apiBaseUrl || \"https://api.openai.com/v1\";\n    const response = await fetch(`${baseUrl}/audio/transcriptions`, {\n      method: \"POST\",\n      headers: {\n        \"Authorization\": `Bearer ${this.config.apiKey}`,\n      },\n      body: formData,\n    });\n\n    if (!response.ok) {\n      const error = await response.text();\n      throw new Error(`Whisper API error: ${error}`);\n    }\n\n    const data = await response.json() as {\n      text: string;\n      language: string;\n      duration: number;\n      segments: Array<{\n        id: number;\n        start: number;\n        end: number;\n        text: string;\n        avg_logprob: number;\n        no_speech_prob: number;\n      }>;\n    };\n\n    // Convert to utterances\n    const utterances: Utterance[] = data.segments.map((seg, i) => ({\n      id: `ut_temp_${String(i).padStart(4, \"0\")}`,\n      index: i,\n      speaker: {\n        id: \"spk_unknown\",\n        name: \"Speaker\",\n      },\n      text: seg.text.trim(),\n      start_ms: Math.round(seg.start * 1000),\n      end_ms: Math.round(seg.end * 1000),\n      duration_ms: Math.round((seg.end - seg.start) * 1000),\n      confidence: {\n        transcription: 1 - seg.no_speech_prob,\n      },\n      language: data.language,\n    }));\n\n    return {\n      utterances,\n      language: data.language,\n      duration_ms: Math.round(data.duration * 1000),\n      processing_time_ms: Date.now() - startTime,\n      model: options?.model || \"whisper-1\",\n    };\n  }\n\n  /**\n   * Run whisper.cpp and capture output\n   */\n  private runWhisper(\n    args: string[],\n    onProgress?: TranscriptionProgressCallback\n  ): Promise<string> {\n    return new Promise((resolve, reject) => {\n      const proc = spawn(this.config.whisperCppPath || \"whisper\", args);\n\n      let stdout = \"\";\n      let stderr = \"\";\n\n      proc.stdout.on(\"data\", (data) => {\n        stdout += data.toString();\n      });\n\n      proc.stderr.on(\"data\", (data) => {\n        const text = data.toString();\n        stderr += text;\n\n        // Parse progress from stderr\n        if (onProgress) {\n          const match = text.match(/progress\\s*=\\s*(\\d+)%/);\n          if (match) {\n            onProgress({\n              percent: parseInt(match[1], 10),\n              current_time_ms: 0,\n              total_time_ms: 0,\n              utterances_processed: 0,\n            });\n          }\n        }\n      });\n\n      proc.on(\"error\", (error) => {\n        reject(new Error(`Failed to run whisper: ${error.message}`));\n      });\n\n      proc.on(\"close\", (code) => {\n        if (code === 0) {\n          resolve(stdout);\n        } else {\n          reject(new Error(`Whisper exited with code ${code}: ${stderr}`));\n        }\n      });\n    });\n  }\n\n  /**\n   * Get model file path\n   */\n  private getModelPath(model: string): string {\n    if (this.config.modelPath) {\n      return this.config.modelPath;\n    }\n    // Default whisper.cpp model location\n    return `ggml-${model}.bin`;\n  }\n\n  /**\n   * Streaming transcription (local only)\n   */\n  async *transcribeStream(\n    input: AudioInput,\n    options?: TranscriptionOptions\n  ): AsyncGenerator<StreamingEvent> {\n    if (this.config.mode === \"api\") {\n      throw new Error(\"Streaming not supported in API mode\");\n    }\n\n    // For now, fall back to batch and emit as single final\n    yield { type: \"started\", session_id: `whisper_${Date.now()}` };\n\n    try {\n      const result = await this.transcribeLocal(input, options);\n\n      for (const utterance of result.utterances) {\n        yield { type: \"final\", utterance };\n      }\n\n      yield { type: \"completed\", result };\n    } catch (error) {\n      yield { type: \"error\", error: error as Error };\n    }\n  }\n}\n\n/**\n * Create a Whisper adapter\n */\nexport function createWhisperAdapter(config?: Partial<WhisperConfig>): WhisperAdapter {\n  return new WhisperAdapter(config);\n}\n", "structuredPatch": [{"oldStart": 6, "oldLines": 7, "newStart": 6, "newLines": 6, "lines": ["  */", " ", " import { spawn } from \"child_process\";", "-import { existsSync, statSync } from \"fs\";", " import type {", "   TranscriptionPort,", "   TranscriptionCapabilities,"]}], "userModified": false, "replaceAll": false}, "tool_use_id": "toolu_014xK1tnhjWvzaUKYgeH1jaX"}}
{"ts": "2025-12-18T17:46:55.000269", "type": "PreToolUse", "session_id": "18b9413c-d1b7-4db7-83e6-73cef8efe045", "agent_session": 0, "data": {"session_id": "18b9413c-d1b7-4db7-83e6-73cef8efe045", "transcript_path": "/home/ygg/.claude/projects/-home-ygg-Workspace-sandbox-marketplaces-claude/18b9413c-d1b7-4db7-83e6-73cef8efe045.jsonl", "cwd": "/home/ygg/Workspace/sandbox/marketplaces/claude/plugins/statusline/hooks", "permission_mode": "bypassPermissions", "hook_event_name": "PreToolUse", "tool_name": "Bash", "tool_input": {"command": "cd /home/ygg/Workspace/sandbox/marketplaces/claude/plugins/transcripts && npx tsc 2>&1", "description": "Compile TypeScript"}, "tool_use_id": "toolu_01RnSdh1XvUSedW9vADriguh"}}
